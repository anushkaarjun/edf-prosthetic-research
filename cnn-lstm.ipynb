# -----------------------
# Imports
# -----------------------
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt
import mne
from torch.optim.lr_scheduler import CyclicLR

# -----------------------
# Config
# -----------------------
DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
DATA_PATH = '/Users/anushkaarjun/Desktop/Outside of School/Prosethic Research Data/files 2'

BATCH_SIZE = 64
EPOCHS = 25
PATIENCE = 10

SLIDING_WINDOW = 320     # 2 sec at 160Hz
WINDOW_STEP = 80         # 75% overlap

N_CLASSES = 3
LR = 1e-3
WEIGHT_DECAY = 1e-4
DROPOUT = 0.4

CLASS_NAMES = [
    'Open Left Fist',
    'Open Right Fist',
    'Close Fists'
]

# -----------------------
# Helper functions
# -----------------------
def get_edf_files(subject_list):
    files = []
    for subj in subject_list:
        subj_folder = os.path.join(DATA_PATH, subj)
        if not os.path.exists(subj_folder):
            print(f"[WARN] Subject folder missing: {subj_folder}")
            continue
        for f in os.listdir(subj_folder):
            if f.lower().endswith('.edf'):
                files.append(os.path.join(subj_folder, f))
    return files


def get_motion_segments(edf_files):
    segments = []
    labels = []

    for f in edf_files:
        print(f"[INFO] Loading {f}...")
        raw = mne.io.read_raw_edf(f, preload=True, verbose=False)
        data = raw.get_data()

        try:
            events, event_id = mne.events_from_annotations(raw)
        except Exception as e:
            print(f"[WARN] Could not extract events from {f}: {e}")
            continue

        run = int(os.path.basename(f).split('R')[1].split('.')[0])

        # Channel-wise normalization
        data = StandardScaler().fit_transform(data.T).T

        label_map = {}

        # ---------- 3-CLASS LABEL MAPPING ----------
        for key, code in event_id.items():

            # OPEN LEFT / OPEN RIGHT
            if run in [3,4,7,8,11,12]:
                if 'T1' in key:
                    label_map[code] = 0  # Open left
                if 'T2' in key:
                    label_map[code] = 1  # Open right

            # CLOSE BOTH FISTS
            elif run in [5,6,9,10,13,14]:
                if 'T1' in key or 'T2' in key:
                    label_map[code] = 2  # Close both

        if not label_map:
            print(f"[WARN] No motion events in run {run}, skipping file.")
            continue

        pre = SLIDING_WINDOW // 2
        post = SLIDING_WINDOW // 2

        for onset, _, code in events:
            if code not in label_map:
                continue

            center = onset
            start = max(center - pre, 0)
            end = min(center + post, data.shape[1])

            if end - start < SLIDING_WINDOW:
                continue

            for s in range(start, end - SLIDING_WINDOW + 1, WINDOW_STEP):
                seg = data[:, s:s + SLIDING_WINDOW].astype(np.float32)
                segments.append(seg)
                labels.append(label_map[code])

    print(f"[INFO] Total motion segments loaded: {len(segments)}")
    return segments, labels


def subjects_with_motion(subjects):
    motion_subjects = []
    for subj in subjects:
        files = get_edf_files([subj])
        segments, _ = get_motion_segments(files)
        if len(segments) > 0:
            motion_subjects.append(subj)
            print(f"[INFO] Subject {subj} has motion events.")
        else:
            print(f"[WARN] Subject {subj} has no motion events and will be skipped.")
    return motion_subjects

# -----------------------
# Dataset
# -----------------------
class PreloadedMotionDataset(Dataset):
    def __init__(self, segments, labels, augment=False):
        self.X = np.array(segments)
        self.y = np.array(labels)
        self.augment = augment

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = self.X[idx]
        y = self.y[idx]

        if self.augment:
            x += 0.02 * np.random.randn(*x.shape)
            x *= np.random.uniform(0.9, 1.1, (x.shape[0], 1))

        x = np.expand_dims(x, 0)
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.int64)

# -----------------------
# Model
# -----------------------
class CNNLSTM(nn.Module):
    def __init__(self, n_channels, n_classes):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, (1, 5), padding=(0, 2))
        self.bn1 = nn.BatchNorm2d(32)

        self.conv2 = nn.Conv2d(32, 64, (n_channels, 5), padding=(0, 2))
        self.bn2 = nn.BatchNorm2d(64)

        self.pool = nn.MaxPool2d((1, 2))
        self.dropout = nn.Dropout(DROPOUT)

        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)
        self.fc = nn.Linear(128, n_classes)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = self.dropout(x)

        b, f, ch, t = x.shape
        x = x.view(b, f * ch, t).permute(0, 2, 1)

        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])
        return x

# -----------------------
# Training / Evaluation
# -----------------------
def compute_class_weights(dataset):
    counts = np.zeros(N_CLASSES)
    for _, y in dataset:
        counts[y.item()] += 1
    counts[counts == 0] = 1
    weights = counts.sum() / (N_CLASSES * counts)
    return torch.tensor(weights, dtype=torch.float32).to(DEVICE)


def evaluate(model, loader):
    model.eval()
    total, correct = 0, 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            out = model(xb)
            preds = out.argmax(1)

            total += yb.size(0)
            correct += (preds == yb).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(yb.cpu().numpy())

    return correct / total * 100, all_preds, all_labels


def train_model(model, train_loader, val_loader, class_weights):
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    scheduler = CyclicLR(optimizer, base_lr=LR / 10, max_lr=LR,
                          step_size_up=200, mode='triangular2')

    best_val, early_stop = 0, 0
    train_acc_list, val_acc_list = [], []

    for epoch in range(EPOCHS):
        model.train()
        total, correct = 0, 0

        for xb, yb in train_loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)

            optimizer.zero_grad()
            out = model(xb)
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()
            scheduler.step()

            preds = out.argmax(1)
            total += yb.size(0)
            correct += (preds == yb).sum().item()

        train_acc = correct / total * 100
        val_acc, _, _ = evaluate(model, val_loader)

        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)

        print(f"[Epoch {epoch+1}/{EPOCHS}] Train: {train_acc:.2f}% | Val: {val_acc:.2f}%")

        if val_acc > best_val:
            best_val = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            early_stop = 0
        else:
            early_stop += 1

        if early_stop >= PATIENCE:
            print("[INFO] Early stopping triggered")
            break

    model.load_state_dict(torch.load('best_model.pth'))
    return model, train_acc_list, val_acc_list

# -----------------------
# Main Pipeline
# -----------------------
SUBJECTS = [f"S{str(i).zfill(3)}" for i in range(1, 110)]

SUBJECTS_WITH_MOTION = subjects_with_motion(SUBJECTS)

train_subj, temp_subj = train_test_split(SUBJECTS_WITH_MOTION, test_size=0.2, random_state=42)
val_subj, test_subj = train_test_split(temp_subj, test_size=0.5, random_state=42)

train_files = get_edf_files(train_subj)
val_files = get_edf_files(val_subj)
test_files = get_edf_files(test_subj)

train_segments, train_labels = get_motion_segments(train_files)
val_segments, val_labels = get_motion_segments(val_files)
test_segments, test_labels = get_motion_segments(test_files)

train_ds = PreloadedMotionDataset(train_segments, train_labels, augment=True)
val_ds = PreloadedMotionDataset(val_segments, val_labels)
test_ds = PreloadedMotionDataset(test_segments, test_labels)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)

class_weights = compute_class_weights(train_ds)

sample_x, _ = train_ds[0]
model = CNNLSTM(sample_x.shape[1], N_CLASSES).to(DEVICE)

model, train_acc_list, val_acc_list = train_model(
    model, train_loader, val_loader, class_weights
)

test_acc, preds, labels = evaluate(model, test_loader)
print(f"\n[RESULT] Test Accuracy: {test_acc:.2f}%")

# -----------------------
# Plots
# -----------------------
plt.figure(figsize=(8,4))
plt.plot(train_acc_list, label='Train')
plt.plot(val_acc_list, label='Validation')
plt.legend()
plt.title('Accuracy')
plt.show()

cm = confusion_matrix(labels, preds)
ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES).plot(cmap='Blues')
plt.show()

print("\nClassification Report:")
print(classification_report(labels, preds, target_names=CLASS_NAMES))

